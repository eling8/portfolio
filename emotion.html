<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Emily Ling - Portfolio</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="index.html">Emily Ling</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="index.html#projects">Projects</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <section class="about-section text-center" id="about">
            <div class="container">
                <img class="img-project" src="assets/img/project-emotion.jpg" alt="" />
                <div class="row">
                    <div class="col-lg-8 mx-auto">
                        </h4>
                        <h1 class="text-white mb-4">From Images to Emotions and Emotions to Images: Toward Emotion-Driven Scenes</h1>
                        <p class="text-white-50">
                            Using CNNs to predict emotion from an image and its scene graph, and reversing this to predict the elements of an image's content from an emotion distribution.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Projects-->
        <section class="projects-section" id="projects">
            <div class="container">
                <div class="row justify-content-center no-gutters mb-5 mb-lg-0">
                    <div class="col-lg-2"></div>
                    <div class="col-lg-8">
                        <p>
                            We were initially inspired by image synthesis models, such as <a href="http://nvidia-research-mingyuliu.com/gaugan/">GauGAN</a>, which can generate a realistic image based on a segmentation map. We were interested in extending such models to synthesize images, including predicting scene components and composition, based on an input emotion.
                        </p>
                        <p>
                            Towards this end, we focused on predicting scene elements given an emotion distribution. Because there are no existing image datasets containing both emotion and scene graph labels, we first needed to create our own. We synthesized the following two datasets:
                        </p> 
                        <ul>
                            <li><b>Dataset A: Ground truth emotion distribution, Synthesized scene graphs</b> We ran a pretrained object detection system on the Emotion6 dataset, which already contains images and ground truth emotion distribution labels. This resulted in a list of object names, confidences, and bounding boxes for each image.</li>
                            <li><b>Dataset B: Synthesized emotion distribution, Ground truth scene graphs.</b> We ran our model trained on the Emotion6 dataset to predict emotion distributions for the Visual Genome images, which already contain scene graph information.</li>
                        </ul>

                         <div class="row justify-content-center no-gutters">
                            <div class="col-lg-9">
                                <img class="img-fluid mb-4" src="assets/img/emotion/disgust-objects.png" alt="" />
                                <p>Input emotion distribution along with word clouds of objects from the ground truth scene graph, and objects predicted by a model trained on synthesized dataset B. This emotion distribution heavily weights “disgust.”</p>

                                <img class="img-fluid mb-4" src="assets/img/emotion/joy-objects.png" alt="" />
                                <p>A second example of input emotion distribution with word clouds of ground truth and predicted objects. This emotion distribution heavily weights “joy.”</p>
                            </div>
                        </div>

                        <p>
                            Our project consisted of the following steps to synthesize datasets and train a model to predict scene elements from emotion:
                        </p>
                        <ol>
                            <li><b>Emotion classification prediction.</b> We experimented with a baseline SVM model, CNN model, and an AlexNet model to predict the emotion class of an image.</li>
                            <li><b>Emotion distribution prediction.</b> In order to better capture a nuanced set of emotions that an image can convey, we shifted to predicting a distribution of emotions rather than a single emotion class. For this task, we created models based on AlexNet, ResNet, and VGG architectures.</li>
                            <li><b>Emotion Stimuli Map (ESM) prediction.</b> We wanted to supplement our emotion distribution predictive model from step 2 with some notion of "attention" to the regions of the image that are more relevant to the emotional distribution. We used the EmotionROI dataset, which pairs Emotion6 images with a ground truth pixel map of the regions that evoked the indicated emotion, called the Emotion Stimuli Map (ESM). We trained a fully convolutional model to predict the ESM from an input image. The results from this model was fed as a feature to the model created in step 2, which was run on the Visual Genome dataset to create dataset B. </li>
                            <li><b>Object prediction.</b> We ran the pretrained YOLO object detection system on the Emotion6 dataset to create dataset A, which augmented existing emotion distribution labels with predicted object information.</li>
                            <li><b>Emotion prediction from objects.</b> We trained a model to predict an emotion distribution from the list of object names contained in a scene. We experimented with a baseline model using a single fully connected layer and pretrained GloVe word embeddings; we also explored RNN and LSTM architectures that take into account object relationships in the scene graph.</li>
                            <li><b>Object prediction from emotion distribution.</b> We constructed a neural network that takes the 7-dimensional emotion distribution vector as input, and outputs a vector containing a confidence rating for each object in the possible object set. The ground truth output is a vector where the confidence rating is the object occurrence count in the image divided by the total object occurrence count in the training set. This normalization was done to encourage less frequent but more salient objects to be predicted. </li>
                        </ol>

                        <div class="row justify-content-center no-gutters">
                            <div class="col-lg-5">
                                <img class="img-fluid mb-4" src="assets/img/emotion/em_roi_pred.png" alt="" />
                                <p>A comparison between the true emotion distribution and the distributions predicted by the ResNet-based and VGG-based models for a variety of test set images.</p>
                            </div>
                            <div class="col-lg-2"></div>
                            <div class="col-lg-5">
                                <img class="img-fluid mb-4" src="assets/img/emotion/image-asset.png" alt="" />
                                <p>A comparison between the ground truth and predicted emotion stimuli maps (ESMs). The differences between the ground truth and predicted maps are displayed with red indicating regions with larger magnitude of difference. Model versions that convert the input image to 32x32 and 64x64 are compared.</p>
                            </div>
                        </div>

                        <p>
                            We successfully explored an initial approach towards translating emotional intent to scene component creation. We generated two synthetic datasets, used them to train a model that generates plausible predictions of objects based on emotion distribution. Additionally, for the task of predicting emotion distribution from an image, we used CNN-based approaches and built a supplementary model that predicted emotion stimuli maps for images; our best-performing model exceeded the performance of prior published work.
                        </p>

                        <a class="btn btn-primary js-scroll-trigger" href="assets/files/emotion-paper.pdf">Download the paper</a>
                    </div>

                    <div class="col-lg-2"></div>
                </div>

                <!-- Additional info -->
                <hr class="proj-divider mt-4" />
                <div class="row justify-content-center no-gutters mt-5">
                    <div class="col-lg-6">
                        <h2>Additional information</h2>
                    </div>
                    <div class="col-lg-6">
                        <p>
                            This was my final project for <i>Convolutional Neural Networks for Visual Recognition (CS231N)</i>, which I took during spring 2019 at Stanford University. The course focused on computer vision techniques, particularly CNN’s.
                        </p>
                        <p>
                            The project was done in partnership with two other students in the course. My primary contributions included: implementing emotion classification SVM baseline; finding and running scene graph generation and object detection models on Emotion6 dataset; implementing baseline and neural network models for object detection prediction from emotion distribution on both synthesized datasets; experimenting with various metrics and normalization types.
                        </p>
                        <p><i>May - June 2019</i></p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact-->
        <section class="contact-section bg-black">
            <div class="container">
                <div class="social d-flex justify-content-center">
                    <a class="mx-2" href="mailto:emilyx.ling@gmail.com"><i class="fas fa-envelope"></i></a>
                    <a class="mx-2" href="https://github.com/eling8"><i class="fab fa-github"></i></a>
                    <a class="mx-2" href="http://instagram.com/emling.photo"><i class="fab fa-instagram"></i></a>
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="footer bg-black small text-center text-white-50"><div class="container">Emily Ling, 2020</div></footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
