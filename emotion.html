<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Emily Ling - Portfolio</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="index.html">Emily Ling</a>
                <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="index.html#projects">Projects</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <section class="about-section text-center" id="about">
            <div class="container">
                <img class="img-project" src="assets/img/project-emotion.jpg" alt="" />
                <div class="row">
                    <div class="col-lg-8 mx-auto">
                        </h4>
                        <h1 class="text-white mb-4">From Images to Emotions and Emotions to Images: Toward Emotion-Driven Scenes</h1>
                        <p class="text-white-50">
                            Using CNNs to predict emotion from an image and its scene graph, and reversing this to predict the elements of an image's content from an emotion distribution.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Projects-->
        <section class="projects-section" id="projects">
            <div class="container">
                <div class="row justify-content-center no-gutters mb-5 mb-lg-0">
                    <div class="col-lg-2"></div>
                    <div class="col-lg-8">
                        <p>
                            Recent methods for semantic image synthesis require a full segmentation map of the scene. We are interested in using the abstracted input of emotion to generate image content. We build upon related work in image emotion classification and image content prediction to enable prediction of scene graph components and composition based on an input emotion. Our baseline approaches map images to a single emotion and we extend upon this to predict more detailed emotion distributions building upon state-of-the-art architectures. We explore object prediction and detection methods to synthesize scene graphs with ground truth emotion distributions and vice-versa. To accomplish our task we use the Emotion6 dataset which maps image to emotional responses and Visual Genome to gather images annotated with scene graphs. From our experiments, we developed an Emotion Stimuli Map predictor and integrate it into an enhanced model to predict emotions distributions with an MSE loss of 0.0159, a model that to predict emotions distributions from objects only with an MSE loss of 0.042, and models to generate objects from emotion distributions on the Visual Genome dataset.
                        </p>
                        <p>
                            In this work, we have explored an initial approach towards a translation from emotional intent to scene component creation. We aimed to leverage emotion-labeled image datasets and scene graph-labeled image datasets to learn mappings between these domains. To do so, we generated proxy datasets based on ground-truth emotions with predicted scene components (objects) and based on ground-truth scene graphs with predicted emotion distributions. This step comprised a substantial portion of our task: learning to predict emotion distributions based on input images. Using CNN-based approaches and building a supplementary model that predicted emotion stimuli maps for images, we achieved our best-performing model which exceeded the performance of prior published work.
                        </p>
                        <p>
                            For the scope of this project, we focus on predicting an image’s emotion distribution given the image and its scene graph, and then reversing that mapping to predict elements of an image’s content given an emotion distribution. To do this, we propose the following process. First, we train an emotion distribution predictor on an image emotion classification dataset, Emotion6. Then, we once we achieve a satisfactory model, we run this model on a dataset with human-annotated scene graphs, such as Visual Genome, to learn a mapping between scene graphs and emotions. Finally, we reverse this mapping to enable prediction of scene graph components based on an input emotion distribution.
                        </p>

                        <div class="row justify-content-center no-gutters">
                            <div class="col-lg-5"><img class="img-fluid mb-4" src="assets/img/emotion/em_roi_pred.png" alt="" /></div>
                            <div class="col-lg-2"></div>
                            <div class="col-lg-5"><img class="img-fluid mb-4" src="assets/img/emotion/image-asset.png" alt="" /></div>
                        </div>
                        <div class="row justify-content-center no-gutters">
                            <div class="col-lg-5"><p>A comparison between the true emotion distribution and the distributions predicted by the ResNet-based and VGG-based models for a variety of test set images.</p></div>
                            <div class="col-lg-2"></div>
                            <div class="col-lg-5"><p>A comparison between the ground truth and predicted emotion stimuli maps (ESMs).The differences between the ground truth and predicted maps are displayed with red indicating regions with larger magnitude of difference. Model versions that convert the input image to 32x32 and 64x64 are compared.</p></div>
                        </div>

                        <a class="btn btn-primary js-scroll-trigger" href="assets/files/emotion-paper.pdf">Download the paper</a>
                    </div>

                    <div class="col-lg-2"></div>
                </div>

                <!-- Additional info -->
                <hr class="proj-divider mt-4" />
                <div class="row justify-content-center no-gutters mt-5">
                    <div class="col-lg-6">
                        <h2>Additional information</h2>
                    </div>
                    <div class="col-lg-6">
                        <p>
                            This was my final project for <i>Convolutional Neural Networks for Visual Recognition (CS231N)</i>, which I took during spring 2019 at Stanford University. The course focused on computer vision techniques, particularly CNN’s.
                        </p>
                        <p>
                            The project was done in partnership with two other students in the course. TODO: DISCUSS MY ROLE
                        </p>
                        <p><i>May - June 2019</i></p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact-->
        <section class="contact-section bg-black">
            <div class="container">
                <div class="social d-flex justify-content-center">
                    <a class="mx-2" href="mailto:emilyx.ling@gmail.com"><i class="fas fa-envelope"></i></a>
                    <a class="mx-2" href="https://github.com/eling8"><i class="fab fa-github"></i></a>
                    <a class="mx-2" href="http://instagram.com/emling.photo"><i class="fab fa-instagram"></i></a>
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="footer bg-black small text-center text-white-50"><div class="container">Emily Ling, 2020</div></footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
